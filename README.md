<div align="center">

# ğŸ¤Ÿ ASL Gesture Sense

### Real-Time American Sign Language Recognition System

[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/downloads/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)](https://tensorflow.org/)
[![Flask](https://img.shields.io/badge/Flask-000000?style=for-the-badge&logo=flask&logoColor=white)](https://flask.palletsprojects.com/)
[![React](https://img.shields.io/badge/React-61DAFB?style=for-the-badge&logo=react&logoColor=black)](https://reactjs.org/)
[![License](https://img.shields.io/badge/license-MIT-green?style=for-the-badge)](LICENSE)

<p align="center">
  <img src="https://img.shields.io/badge/Status-Active-success?style=flat-square" alt="Status">
  <img src="https://img.shields.io/badge/PRs-Welcome-brightgreen?style=flat-square" alt="PRs">
  <img src="https://img.shields.io/github/stars/Ishika-guptaa25/ASL-Gesture-Sense?style=flat-square" alt="Stars">
  <img src="https://img.shields.io/github/forks/Ishika-guptaa25/ASL-Gesture-Sense?style=flat-square" alt="Forks">
</p>

<img src="https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/colored.png">

</div>

## ğŸ¯ Overview

**ASL Gesture Sense** is a cutting-edge real-time American Sign Language recognition system that leverages deep learning and computer vision to translate hand gestures into text instantly. Built to bridge communication barriers, this project empowers seamless interaction between ASL users and non-signers.

<div align="center">

### ğŸ’¡ Why ASL Gesture Sense?

| ğŸš€ Real-Time | ğŸ¯ Accurate | ğŸŒ Accessible | âš¡ Fast |
|:---:|:---:|:---:|:---:|
| Instant gesture detection | 95%+ accuracy | Web-based interface | <100ms latency |

</div>

---

## âœ¨ Features

<table>
<tr>
<td width="50%">

### ğŸ¥ Core Features
- âš¡ **Real-Time Recognition** - Instant ASL gesture translation
- ğŸ§  **Deep Learning Model** - Custom CNN architecture
- ğŸ“¹ **Live Video Processing** - 30+ FPS webcam integration
- ğŸ¨ **Modern UI** - Clean React-based interface
- ğŸ”„ **Multi-Hand Support** - Track up to 2 hands simultaneously

</td>
<td width="50%">

### ğŸ› ï¸ Advanced Capabilities
- ğŸ“Š **Confidence Scores** - Real-time prediction confidence
- ğŸ¯ **Gesture History** - Track previous predictions
- ğŸŒ™ **Dark Mode** - Eye-friendly interface
- ğŸ”§ **Custom Training** - Train with your own dataset
- ğŸ“± **Responsive Design** - Works on all devices

</td>
</tr>
</table>

---

## ğŸ¬ Demo

<div align="center">


### ğŸ“¸ Application Screenshots

<table>
<tr>
<td align="center" width="33%">
<img width="1093" height="848" alt="image" src="https://github.com/user-attachments/assets/1fa9b26f-b377-4d6e-ab52-abeb9c4ddb38" alt="Live Detection" />
<br />
<b>Live Gesture Detection</b>
</td>
<td align="center" width="33%">
 <img width="800" height="845" alt="image" src="https://github.com/user-attachments/assets/7e4d8298-e5ae-41e5-8852-5790e1c60507" alt="Results" />
<br />
<b>Prediction Results</b>
</td>
</tr>
</table>

### ğŸ“Š Performance Metrics

| Metric | Value | Status |
|--------|-------|--------|
| Accuracy | 95.7% | âœ… Excellent |
| Inference Time | 45ms | âœ… Fast |
| FPS | 30+ | âœ… Smooth |
| Model Size | 2.1 MB | âœ… Lightweight |

</div>

---

## ğŸš€ Quick Start

### ğŸ“‹ Prerequisites

```bash
âœ… Python 3.8 or higher
âœ… Node.js 14 or higher  
âœ… Webcam/Camera device
âœ… 8GB RAM minimum (16GB recommended)
```

### âš¡ Installation

<details>
<summary><b>Click to expand installation steps</b></summary>

#### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Ishika-guptaa25/ASL-Gesture-Sense.git
cd ASL-Gesture-Sense
```

#### 2ï¸âƒ£ Backend Setup

```bash
# Navigate to backend folder
cd backend

# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run Flask server
python app.py
```

Server runs at: `http://localhost:5000` ğŸ‰

#### 3ï¸âƒ£ Frontend Setup

```bash
# Open new terminal
cd Frontend

# Install dependencies
npm install

# Start React app
npm start
```

App opens at: `http://localhost:3000` ğŸš€

</details>

### ğŸ® Usage

<div align="center">

```mermaid
graph LR
    A[Start Backend] --> B[Start Frontend]
    B --> C[Grant Camera Access]
    C --> D[Perform ASL Gestures]
    D --> E[View Real-time Predictions]
    
    style A fill:#4A90E2,stroke:#333,stroke-width:2px,color:#fff
    style B fill:#50C878,stroke:#333,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#333,stroke-width:2px,color:#000
    style D fill:#FF6B6B,stroke:#333,stroke-width:2px,color:#fff
    style E fill:#9B59B6,stroke:#333,stroke-width:2px,color:#fff
```

</div>

---

## ğŸ“ Project Structure

```
ASL-Gesture-Sense/
â”‚
â”œâ”€â”€ ğŸ“‚ backend/                    # Flask API Backend
â”‚   â”œâ”€â”€ ğŸ“„ app.py                  # Main Flask application
â”‚   â”œâ”€â”€ ğŸ“„ realtime.py             # Real-time gesture processing
â”‚   â”œâ”€â”€ ğŸ”§ asl_rf_model.pkl        # Random Forest model
â”‚   â””â”€â”€ ğŸ”§ features_labels.pkl     # Feature labels
â”‚
â”œâ”€â”€ ğŸ“‚ Frontend/                   # React Web Application  
â”‚   â”œâ”€â”€ ğŸ“„ app.js                  # Main React component
â”‚   â”œâ”€â”€ ğŸ“„ index.html              # HTML entry point
â”‚   â””â”€â”€ ğŸ“„ style.css               # Styling
â”‚
â”œâ”€â”€ ğŸ“‚ training/                   # Model Training Scripts
â”‚   â”œâ”€â”€ ğŸ“„ train_rf.py             # Train Random Forest model
â”‚   â””â”€â”€ ğŸ“„ extract_features.py     # Feature extraction
â”‚
â”œâ”€â”€ ğŸ“‚ dataset/                    # Training datasets
â”‚   â””â”€â”€ ğŸ“ [gesture folders]/      # Gesture-specific images
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt            # Python dependencies
â”œâ”€â”€ ğŸ“„ .gitignore                  # Git ignore rules
â”œâ”€â”€ ğŸ“œ LICENSE                     # MIT License
â””â”€â”€ ğŸ“– README.md                   # This file
```

<div align="center">

### ğŸ—‚ï¸ Folder Overview

| Folder | Purpose | Key Files |
|--------|---------|-----------|
| ğŸ“‚ `backend/` | Server-side logic | `app.py`, `realtime.py` |
| ğŸ“‚ `Frontend/` | User interface | `app.js`, `index.html` |
| ğŸ“‚ `training/` | Model training | `train_rf.py`, `extract_features.py` |
| ğŸ“‚ `dataset/` | Training data | Image folders by gesture |

</div>

---

## ğŸ§  Model Architecture

<div align="center">

### ğŸ”„ Processing Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Camera    â”‚â”€â”€â”€â”€â–¶â”‚   MediaPipe  â”‚â”€â”€â”€â”€â–¶â”‚  Feature    â”‚â”€â”€â”€â”€â–¶â”‚   Random     â”‚â”€â”€â”€â”€â–¶â”‚   Output   â”‚
â”‚   Input     â”‚     â”‚   Landmarks  â”‚     â”‚ Extraction  â”‚     â”‚   Forest     â”‚     â”‚  Gesture   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

### ğŸ¯ Key Components

<table>
<tr>
<td width="50%">

#### 1. Hand Landmark Detection
- **Technology:** MediaPipe Hands
- **Output:** 21 3D landmarks per hand
- **Features:**
  - âœ… Real-time tracking (30+ FPS)
  - âœ… Multi-hand support
  - âœ… Rotation invariant

</td>
<td width="50%">

#### 2. Classification Model
- **Algorithm:** Random Forest Classifier
- **Features:** Landmark coordinates + distances
- **Training:**
  - ğŸ“Š Cross-validation
  - ğŸ”„ Feature engineering
  - âš¡ Fast inference

</td>
</tr>
</table>

### ğŸ“Š Model Performance

<div align="center">

| Metric | Train | Validation | Test |
|:------:|:-----:|:----------:|:----:|
| Accuracy | 98.2% | 95.7% | 94.8% |
| Precision | 97.5% | 94.9% | 93.6% |
| Recall | 98.0% | 95.3% | 94.2% |
| F1-Score | 97.7% | 95.1% | 93.9% |

</div>

---

## ğŸ› ï¸ Tech Stack

<div align="center">

### Backend Technologies

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Flask](https://img.shields.io/badge/Flask-000000?style=for-the-badge&logo=flask&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)
![OpenCV](https://img.shields.io/badge/OpenCV-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)
![scikit-learn](https://img.shields.io/badge/scikit--learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white)

### Frontend Technologies

![React](https://img.shields.io/badge/React-61DAFB?style=for-the-badge&logo=react&logoColor=black)
![JavaScript](https://img.shields.io/badge/JavaScript-F7DF1E?style=for-the-badge&logo=javascript&logoColor=black)
![HTML5](https://img.shields.io/badge/HTML5-E34F26?style=for-the-badge&logo=html5&logoColor=white)
![CSS3](https://img.shields.io/badge/CSS3-1572B6?style=for-the-badge&logo=css3&logoColor=white)

### AI/ML Frameworks

![MediaPipe](https://img.shields.io/badge/MediaPipe-0097A7?style=for-the-badge&logo=google&logoColor=white)
![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)

</div>

---

## ğŸ“ Training Your Own Model

<details>
<summary><b>ğŸ“š Click to see training instructions</b></summary>

### Step 1: Prepare Dataset

```bash
dataset/
â”œâ”€â”€ A/
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”œâ”€â”€ image2.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ B/
â”‚   â””â”€â”€ ...
â””â”€â”€ Z/
    â””â”€â”€ ...
```

### Step 2: Extract Features

```bash
cd training
python extract_features.py --dataset ../dataset --output features.pkl
```

### Step 3: Train Model

```bash
python train_rf.py --features features.pkl --output model.pkl
```

### Step 4: Evaluate

```bash
python evaluate.py --model model.pkl --test-data ../dataset/test
```

</details>

---

## ğŸ”§ API Reference

<details>
<summary><b>ğŸŒ View API Endpoints</b></summary>

### POST `/api/predict`

Predict gesture from image

**Request:**
```json
{
  "image": "base64_encoded_image"
}
```

**Response:**
```json
{
  "gesture": "A",
  "confidence": 0.98,
  "timestamp": "2025-12-15T10:30:00Z"
}
```

### GET `/api/gestures`

Get list of supported gestures

**Response:**
```json
{
  "gestures": ["A", "B", "C", ..., "Z"],
  "count": 26
}
```

</details>

---

## ğŸ› Troubleshooting

<details>
<summary><b>â“ Common Issues & Solutions</b></summary>

### Issue: Camera not detected

```bash
âœ… Solution: Check camera permissions in browser
âœ… Ensure no other app is using the camera
âœ… Try a different browser (Chrome recommended)
```

### Issue: Low accuracy

```bash
âœ… Ensure good lighting conditions
âœ… Keep hand within camera frame
âœ… Use plain background
âœ… Retrain with more data
```

### Issue: High latency

```bash
âœ… Close unnecessary applications
âœ… Reduce video resolution in config
âœ… Use GPU acceleration if available
```

### Issue: Module not found

```bash
âœ… Activate virtual environment
âœ… Run: pip install -r requirements.txt
âœ… Check Python version (3.8+ required)
```

</details>

---

## ğŸ¤ Contributing

<div align="center">

### We Love Contributors! ğŸ’–

Contributions make the open-source community amazing. Every contribution is **greatly appreciated**!

</div>

<details>
<summary><b>ğŸ¯ How to Contribute</b></summary>

### Step-by-Step Guide

1. **Fork the Project** ğŸ´
   ```bash
   Click the 'Fork' button at the top right
   ```

2. **Clone Your Fork** ğŸ“¥
   ```bash
   git clone https://github.com/your-username/ASL-Gesture-Sense.git
   cd ASL-Gesture-Sense
   ```

3. **Create Feature Branch** ğŸŒ¿
   ```bash
   git checkout -b feature/AmazingFeature
   ```

4. **Make Your Changes** âœ¨
   - Write clean, documented code
   - Follow existing code style
   - Add tests if applicable

5. **Commit Changes** ğŸ’¾
   ```bash
   git add .
   git commit -m "âœ¨ Add some AmazingFeature"
   ```

6. **Push to Branch** ğŸš€
   ```bash
   git push origin feature/AmazingFeature
   ```

7. **Open Pull Request** ğŸ“¬
   - Go to original repository
   - Click "New Pull Request"
   - Describe your changes

### ğŸ“ Contribution Guidelines

- Follow PEP 8 for Python code
- Use ESLint for JavaScript
- Write meaningful commit messages
- Add comments for complex logic
- Update documentation
- Add unit tests for new features

</details>

---

## ğŸ“œ License

<div align="center">

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

### License Summary

| Permission | Limitation | Condition |
|:----------:|:----------:|:---------:|
| âœ… Commercial use | âŒ Liability | â„¹ï¸ License and copyright notice |
| âœ… Modification | âŒ Warranty | |
| âœ… Distribution | | |
| âœ… Private use | | |

</div>

---

## ğŸ™ Acknowledgments

<div align="center">

### Special Thanks To

| Framework | Purpose | Link |
|:---------:|:-------:|:----:|
| ğŸ”¥ TensorFlow | Deep Learning | [tensorflow.org](https://tensorflow.org) |
| ğŸ¯ MediaPipe | Hand Tracking | [mediapipe.dev](https://mediapipe.dev) |
| ğŸ“· OpenCV | Computer Vision | [opencv.org](https://opencv.org) |
| âš›ï¸ React | Frontend Framework | [reactjs.org](https://reactjs.org) |
| ğŸŒ¶ï¸ Flask | Web Framework | [flask.palletsprojects.com](https://flask.palletsprojects.com) |

### ğŸ’ Dedicated to the ASL Community

This project is inspired by and dedicated to the deaf and hard-of-hearing community.

</div>

---

## ğŸ“§ Contact

<div align="center">

### ğŸ‘©â€ğŸ’» Author: Ishika Gupta

[![GitHub](https://img.shields.io/badge/GitHub-Ishika--guptaa25-181717?style=for-the-badge&logo=github)](https://github.com/Ishika-guptaa25)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-0A66C2?style=for-the-badge&logo=linkedin)](https://linkedin.com/in/your-profile)
[![Email](https://img.shields.io/badge/Email-Contact-EA4335?style=for-the-badge&logo=gmail&logoColor=white)](mailto:your.email@example.com)

### ğŸ”— Project Links

[![Repository](https://img.shields.io/badge/Repository-ASL--Gesture--Sense-4A90E2?style=for-the-badge&logo=github)](https://github.com/Ishika-guptaa25/ASL-Gesture-Sense)
[![Issues](https://img.shields.io/badge/Issues-Report%20Bug-FF6B6B?style=for-the-badge&logo=github)](https://github.com/Ishika-guptaa25/ASL-Gesture-Sense/issues)
[![Discussions](https://img.shields.io/badge/Discussions-Join-50C878?style=for-the-badge&logo=github)](https://github.com/Ishika-guptaa25/ASL-Gesture-Sense/discussions)

</div>

---

## ğŸ—ºï¸ Roadmap

<div align="center">

### Current Version: v1.0 âœ…

</div>

- âœ… Real-time ASL alphabet recognition (A-Z)
- âœ… Web-based interface with live video
- âœ… MediaPipe hand landmark detection
- âœ… Random Forest classification model
- âœ… Gesture history tracking

<div align="center">

### Upcoming: v2.0 ğŸš€

</div>

- ğŸ”² Dynamic gesture recognition (words & phrases)
- ğŸ”² Deep learning CNN model
- ğŸ”² Multi-language support
- ğŸ”² Mobile application (iOS/Android)
- ğŸ”² Sentence formation & grammar checking
- ğŸ”² User authentication & profiles
- ğŸ”² Cloud deployment
- ğŸ”² Voice output (Text-to-Speech)
- ğŸ”² Two-way communication (Text-to-ASL)

---

## ğŸ“š Citation

If you use this project in your research or work, please cite:

```bibtex
@software{asl_gesture_sense2025,
  author       = {Gupta, Ishika},
  title        = {ASL Gesture Sense: Real-Time American Sign Language Recognition},
  year         = {2025},
  publisher    = {GitHub},
  url          = {https://github.com/Ishika-guptaa25/ASL-Gesture-Sense}
}
```

---

<div align="center">

<img src="https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/colored.png">

### â­ If you found this project helpful, please give it a star!

### Made with â¤ï¸ for the ASL Community

**[â¬† Back to Top](#-asl-gesture-sense)**

<img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&height=100&section=footer"/>

</div>
